{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e",
    "tags": []
   },
   "source": [
    "# AAMR - Custom Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.10.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is used demonstrate custom model training capability using Tensorflow Recommenders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Objective\n",
    "\n",
    "1. Load custom data into tensorflow dataset\n",
    "2. Create a custom model \n",
    "3. Validate custom model recommendations\n",
    "4. Save model to gcs location for deployment and serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "1. Medications master set\n",
    "2. Patient Current medications list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), \n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow\n",
    "!pip install -q scann\n",
    "!pip install tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "#! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY",
    "tags": []
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"aamr-432116\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "**- Authenticate the Cloud SDK with your credentials :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "authlibtitle"
   },
   "source": [
    "**- Authenticate code and libraries with your credentials :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "authlibcode"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth application-default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "**- Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets.\n",
    "\n",
    "- *{Note to notebook author: For any user-provided strings that need to be unique (like bucket names or model ID's), append \"-unique\" to the end so proper testing can occur}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "#BUCKET_URI = f\"gs://tf-recommenders-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    "#BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "#! gsutil mb -p {PROJECT_ID} gs://tf-recommenders-aamr-432116-unique/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf",
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "Let's first have a look at the data.\n",
    "\n",
    "We will load the Custom data set from csv files into [Tensorflow Datasets](https://www.tensorflow.org/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med_data = pd.read_csv(\n",
    "    \"data/Medicine_Details.csv\",\n",
    "    names=[\"Medicine_Name\",\"Composition\",\"Uses\",\"Side_effects\",\"Image_URL\",\"Manufacturer\",\"Excellent_Review_Percentage\",\"Average_Review_Percentage\",\"Poor_Review_Percentage\"],\n",
    "    dtype = str)\n",
    "\n",
    "#med_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_health_data = pd.read_csv(\n",
    "    \"data/patients_health_mapped_data.csv\",\n",
    "    names=[\"Diabetic\",\"AlcoholLevel\",\"HeartRate\",\"BloodOxygenLevel\",\"BodyTemperature\",\"Weight\",\"MRI_Delay\",\"Medicine_Name\",\"Dosage_in_mg\",\"Age\",\"Education_Level\",\"Dominant_Hand\",\"Gender\",\"Family_History\",\"Smoking_Status\",\"APOE_E4\",\"Physical_Activity\",\"Depression_Status\",\"Cognitive_Test_Scores\",\"Medication_History\",\"Nutrition_Diet\",\"Sleep_Quality\",\"Chronic_Health_Conditions\",\"Dementia\",\"Patient_Id\"],\n",
    "    dtype = str)\n",
    "\n",
    "#patient_health_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# All available Medication data\n",
    "med_dataset = tf.data.Dataset.from_tensor_slices(dict(med_data))\n",
    "\n",
    "# Patient associated medication data\n",
    "patient_dataset = tf.data.Dataset.from_tensor_slices(dict(patient_health_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "#ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "#movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Average_Review_Percentage': b'56',\n",
      " 'Composition': b'Bevacizumab (400mg)',\n",
      " 'Excellent_Review_Percentage': b'22',\n",
      " 'Image_URL': b'https://onemg.gumlet.io/l_watermark_346,w_480,h_480/a_ignore'\n",
      "              b',w_480,h_480,c_fit,q_auto,f_auto/f5a26c491e4d48199ab116a69a9'\n",
      "              b'69be3.jpg',\n",
      " 'Manufacturer': b'Roche Products India Pvt Ltd',\n",
      " 'Medicine_Name': b'Avastin 400mg Injection',\n",
      " 'Poor_Review_Percentage': b'22',\n",
      " 'Side_effects': b'Rectal bleeding Taste change Headache Nosebleeds Back pain D'\n",
      "                 b'ry skin High blood pressure Protein in urine Inflammation of'\n",
      "                 b' the nose',\n",
      " 'Uses': b' Cancer of colon and rectum Non-small cell lung cancer Kidney cancer'\n",
      "         b' Brain tumor Ovarian cancer Cervical cancer'}\n"
     ]
    }
   ],
   "source": [
    "for x in med_dataset.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'APOE_E4': b'Negative',\n",
      " 'Age': b'60',\n",
      " 'AlcoholLevel': b'0.08497362913',\n",
      " 'BloodOxygenLevel': b'96.23074296',\n",
      " 'BodyTemperature': b'36.22485168',\n",
      " 'Chronic_Health_Conditions': b'Diabetes',\n",
      " 'Cognitive_Test_Scores': b'10',\n",
      " 'Dementia': b'0',\n",
      " 'Depression_Status': b'No',\n",
      " 'Diabetic': b'1',\n",
      " 'Dominant_Hand': b'Left',\n",
      " 'Dosage_in_mg': b'12',\n",
      " 'Education_Level': b'Primary School',\n",
      " 'Family_History': b'No',\n",
      " 'Gender': b'Female',\n",
      " 'HeartRate': b'98',\n",
      " 'MRI_Delay': b'36.42102798',\n",
      " 'Medication_History': b'No',\n",
      " 'Medicine_Name': b'Acnesol Gel',\n",
      " 'Nutrition_Diet': b'Low-Carb Diet',\n",
      " 'Patient_Id': b'10001',\n",
      " 'Physical_Activity': b'Sedentary',\n",
      " 'Sleep_Quality': b'Poor',\n",
      " 'Smoking_Status': b'Current Smoker',\n",
      " 'Weight': b'57.56397754'}\n"
     ]
    }
   ],
   "source": [
    "for x in patient_dataset.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_dataset = patient_dataset.map(lambda x: {\n",
    "    \"Medicine_Name\": x[\"Medicine_Name\"],\n",
    "    \"Patient_Id\": x[\"Patient_Id\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med_dataset = med_dataset.map(lambda x: x[\"Medicine_Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = patient_dataset.shuffle(1000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(800)\n",
    "test = shuffled.skip(200).take(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'A Doxid 100mg Capsule', b'A Ret HC Cream', b'A-CN Gel',\n",
       "       b'A-Ret 0.025% Gel', b'A-Ret 0.05% Gel', b'A-Ret 0.1% Gel',\n",
       "       b'A-Ret 0.5% Cream', b'AA 5 Tablet', b'AB Phylline Capsule',\n",
       "       b'AB Phylline N Tablet'], dtype=object)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_titles = med_dataset.batch(1_000)\n",
    "patient_dataset_ids = patient_dataset.batch(1_000_000).map(lambda x: x[\"Patient_Id\"])\n",
    "\n",
    "unique_med_titles = np.unique(np.concatenate(list(med_titles)))\n",
    "unique_patient_dataset_ids = np.unique(np.concatenate(list(patient_dataset_ids)))\n",
    "\n",
    "unique_med_titles[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'10001', b'10002', b'10003', b'10004', b'10005', b'10006',\n",
       "       b'10007', b'10008', b'10009', b'10010'], dtype=object)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_patient_dataset_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_patient_dataset_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementing a model\n",
    "\n",
    "Choosing the architecture of our model is a key part of modelling.\n",
    "\n",
    "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The query tower\n",
    "\n",
    "Let's start with the query tower.\n",
    "\n",
    "The first step is to decide on the dimensionality of the query and candidate representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_med_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_patient_dataset_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_patient_dataset_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The candidate tower\n",
    "\n",
    "We can do the same with the candidate tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_med_titles, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_med_titles) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In our training data we have positive (user, medication) pairs. To figure out how good our model is, we need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
    "\n",
    "To do this, we can use the `tfrs.metrics.FactorizedTopK` metric. The metric has one required argument: the dataset of candidates that are used as implicit negatives for evaluation.\n",
    "\n",
    "In our case, that's the `meds` dataset, converted into embeddings via our movie model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=med_dataset.batch(128).map(med_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "The next component is the loss used to train our model. TFRS has several loss layers and tasks to make this easy.\n",
    "\n",
    "In this instance, we'll make use of the `Retrieval` task object: a convenience wrapper that bundles together the loss function and metric computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full model\n",
    "\n",
    "We can now put it all together into a model. \n",
    "TFRS exposes a base model class (`tfrs.models.Model`) which streamlines building models: all we need to do is to set up the components in the `__init__` method, and implement the `compute_loss` method, taking in the raw features and returning a loss value.\n",
    "\n",
    "The base model will then take care of creating the appropriate training loop to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MedRecomModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_med_model, med_model):\n",
    "    super().__init__()\n",
    "    self.med_model: tf.keras.Model = med_model\n",
    "    self.user_med_model: tf.keras.Model = user_med_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_med_embeddings = self.user_med_model(features[\"Patient_Id\"])\n",
    "    # And pick out the med features and pass them into the med model,\n",
    "    # getting embeddings back.\n",
    "    positive_med_embeddings = self.med_model(features[\"Medicine_Name\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_med_embeddings, positive_med_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fitting and evaluating\n",
    "\n",
    "After defining the model, we can use standard Keras fitting and evaluation routines to fit and evaluate the model.\n",
    "\n",
    "Let's first instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MedRecomModel(user_med_model, med_model)\n",
    "#Pick righ optimizer for training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then shuffle, batch, and cache the training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(1000).batch(10).cache()\n",
    "cached_test = test.batch(100).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "80/80 [==============================] - 15s 172ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0288 - factorized_top_k/top_5_categorical_accuracy: 0.0400 - factorized_top_k/top_10_categorical_accuracy: 0.0538 - factorized_top_k/top_50_categorical_accuracy: 0.0775 - factorized_top_k/top_100_categorical_accuracy: 0.0950 - loss: 23.0295 - regularization_loss: 0.0000e+00 - total_loss: 23.0295\n",
      "Epoch 2/3\n",
      "80/80 [==============================] - 13s 165ms/step - factorized_top_k/top_1_categorical_accuracy: 0.3862 - factorized_top_k/top_5_categorical_accuracy: 0.4725 - factorized_top_k/top_10_categorical_accuracy: 0.5063 - factorized_top_k/top_50_categorical_accuracy: 0.5562 - factorized_top_k/top_100_categorical_accuracy: 0.5962 - loss: 22.8161 - regularization_loss: 0.0000e+00 - total_loss: 22.8161\n",
      "Epoch 3/3\n",
      "80/80 [==============================] - 14s 172ms/step - factorized_top_k/top_1_categorical_accuracy: 0.7100 - factorized_top_k/top_5_categorical_accuracy: 0.7912 - factorized_top_k/top_10_categorical_accuracy: 0.8100 - factorized_top_k/top_50_categorical_accuracy: 0.8775 - factorized_top_k/top_100_categorical_accuracy: 0.8988 - loss: 20.8743 - regularization_loss: 0.0000e+00 - total_loss: 20.8743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 86848), started 13:58:05 ago. (Use '!kill 86848' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9ee70c65e5b650a4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9ee70c65e5b650a4\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3,callbacks=[tensorboard_callback])\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cached_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 258ms/step - factorized_top_k/top_1_categorical_accuracy: 0.7750 - factorized_top_k/top_5_categorical_accuracy: 0.8300 - factorized_top_k/top_10_categorical_accuracy: 0.8600 - factorized_top_k/top_50_categorical_accuracy: 0.9650 - factorized_top_k/top_100_categorical_accuracy: 0.9650 - loss: 417.0653 - regularization_loss: 0.0000e+00 - total_loss: 417.0653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.7749999761581421,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.8299999833106995,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.8600000143051147,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.9649999737739563,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.9649999737739563,\n",
       " 'loss': 416.4399108886719,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 416.4399108886719}"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making predictions\n",
    "\n",
    "Now that we have a model, we would like to be able to make predictions. We can use the `tfrs.layers.factorized_top_k.BruteForce` layer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 10025: [b'Q-Mind SR 200 Tablet' b'Aquazide 25 Tablet' b'Asomex 5 Tablet'\n",
      " b'Myelogen PG 75 Tablet SR' b'Lupenox 40mg Injection'\n",
      " b'Pregalin X 75 Capsule' b'Qutiwel 50mg Tablet SR' b'Nadoxin Ointment'\n",
      " b'Nexpro Fast 20 Tablet' b'Acenac-N Tablet PR']\n"
     ]
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_med_model)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((med_dataset.batch(100), med_dataset.batch(100).map(model.med_model)))\n",
    ")\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = index(tf.constant([\"2345\"]))\n",
    "print(f\"Recommendations for user 10025: {titles[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model serving\n",
    "\n",
    "After the model is trained, we need a way to deploy it.\n",
    "\n",
    "In a two-tower retrieval model, serving has two components:\n",
    "\n",
    "- a serving query model, taking in features of the query and transforming them into a query embedding, and\n",
    "- a serving candidate model. This most often takes the form of an approximate nearest neighbours (ANN) index which allows fast approximate lookup of candidates in response to a query produced by the query model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Export the query model.\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "  path = os.path.join(tmp, \"model\")\n",
    "\n",
    "  # Save the index.\n",
    "  tf.saved_model.save(index, path)\n",
    "\n",
    "  # Load it back; can also be done in TensorFlow Serving.\n",
    "  loaded = tf.saved_model.load(path)\n",
    "\n",
    "  # Pass a user id in, get top predicted movie titles back.\n",
    "  scores, titles = loaded([\"10042\"])\n",
    "\n",
    "  print(f\"Recommendations: {titles[0][:3]}\")\n",
    "  dst_path = \"/tmp/vc-model-med-recomm-2\"\n",
    "  os.rename(path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/vc-model-med-recomm-2/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "Copying file:///tmp/vc-model-med-recomm-2/fingerprint.pb [Content-Type=application/octet-stream]...\n",
      "Copying file:///tmp/vc-model-med-recomm-2/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "Copying file:///tmp/vc-model-med-recomm-2/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "- [4 files][  1.4 MiB/  1.4 MiB]                                                \n",
      "Operation completed over 4 objects/1.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp -r \"/tmp/vc-model-med-recomm-2\" \"gs://vc-model-training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete endpoint resource\n",
    "# e.g. `endpoint.delete()`\n",
    "\n",
    "# Delete model resource\n",
    "# e.g. `model.delete()`\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = True\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
