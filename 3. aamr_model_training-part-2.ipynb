{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e",
    "tags": []
   },
   "source": [
    "# AAMR - Custom Model Training - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.10.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is used demonstrate custom model training capability using Tensorflow Recommenders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Objective\n",
    "\n",
    "1. Load custom data into tensorflow dataset\n",
    "2. Create a custom model \n",
    "3. Validate custom model recommendations\n",
    "4. Save model to gcs location for deployment and serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "1. Medications master set\n",
    "2. Patient Current medications list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), \n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow\n",
    "!pip install -q scann\n",
    "!pip install tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "#! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf",
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "Let's first have a look at the data.\n",
    "\n",
    "We will load the Custom data set from csv files into [Tensorflow Datasets](https://www.tensorflow.org/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med_data = pd.read_csv(\n",
    "    \"data/Medicine_Details.csv\",\n",
    "    names=[\"Medicine_Name\",\"Composition\",\"Uses\",\"Side_effects\",\"Image_URL\",\"Manufacturer\",\"Excellent_Review_Percentage\",\"Average_Review_Percentage\",\"Poor_Review_Percentage\"],\n",
    "    dtype = str)\n",
    "\n",
    "#med_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_health_data = pd.read_csv(\n",
    "    \"data/patients_health_mapped_data.csv\",\n",
    "    names=[\"Diabetic\",\"AlcoholLevel\",\"HeartRate\",\"BloodOxygenLevel\",\"BodyTemperature\",\"Weight\",\"MRI_Delay\",\"Medicine_Name\",\"Dosage_in_mg\",\"Age\",\"Education_Level\",\"Dominant_Hand\",\"Gender\",\"Family_History\",\"Smoking_Status\",\"APOE_E4\",\"Physical_Activity\",\"Depression_Status\",\"Cognitive_Test_Scores\",\"Medication_History\",\"Nutrition_Diet\",\"Sleep_Quality\",\"Chronic_Health_Conditions\",\"Dementia\",\"Patient_Id\"],\n",
    "    dtype = str)\n",
    "\n",
    "#patient_health_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# All available Medication data\n",
    "med_dataset = tf.data.Dataset.from_tensor_slices(dict(med_data))\n",
    "\n",
    "# Patient associated medication data\n",
    "patient_dataset = tf.data.Dataset.from_tensor_slices(dict(patient_health_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "#ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "#movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Average_Review_Percentage': b'56',\n",
      " 'Composition': b'Bevacizumab (400mg)',\n",
      " 'Excellent_Review_Percentage': b'22',\n",
      " 'Image_URL': b'https://onemg.gumlet.io/l_watermark_346,w_480,h_480/a_ignore'\n",
      "              b',w_480,h_480,c_fit,q_auto,f_auto/f5a26c491e4d48199ab116a69a9'\n",
      "              b'69be3.jpg',\n",
      " 'Manufacturer': b'Roche Products India Pvt Ltd',\n",
      " 'Medicine_Name': b'Avastin 400mg Injection',\n",
      " 'Poor_Review_Percentage': b'22',\n",
      " 'Side_effects': b'Rectal bleeding Taste change Headache Nosebleeds Back pain D'\n",
      "                 b'ry skin High blood pressure Protein in urine Inflammation of'\n",
      "                 b' the nose',\n",
      " 'Uses': b' Cancer of colon and rectum Non-small cell lung cancer Kidney cancer'\n",
      "         b' Brain tumor Ovarian cancer Cervical cancer'}\n"
     ]
    }
   ],
   "source": [
    "for x in med_dataset.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'APOE_E4': b'Negative',\n",
      " 'Age': b'60',\n",
      " 'AlcoholLevel': b'0.08497362913',\n",
      " 'BloodOxygenLevel': b'96.23074296',\n",
      " 'BodyTemperature': b'36.22485168',\n",
      " 'Chronic_Health_Conditions': b'Diabetes',\n",
      " 'Cognitive_Test_Scores': b'10',\n",
      " 'Dementia': b'0',\n",
      " 'Depression_Status': b'No',\n",
      " 'Diabetic': b'1',\n",
      " 'Dominant_Hand': b'Left',\n",
      " 'Dosage_in_mg': b'12',\n",
      " 'Education_Level': b'Primary School',\n",
      " 'Family_History': b'No',\n",
      " 'Gender': b'Female',\n",
      " 'HeartRate': b'98',\n",
      " 'MRI_Delay': b'36.42102798',\n",
      " 'Medication_History': b'No',\n",
      " 'Medicine_Name': b'Acnesol Gel',\n",
      " 'Nutrition_Diet': b'Low-Carb Diet',\n",
      " 'Patient_Id': b'10001',\n",
      " 'Physical_Activity': b'Sedentary',\n",
      " 'Sleep_Quality': b'Poor',\n",
      " 'Smoking_Status': b'Current Smoker',\n",
      " 'Weight': b'57.56397754'}\n"
     ]
    }
   ],
   "source": [
    "for x in patient_dataset.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_dataset_select = patient_dataset.map(lambda x: {\n",
    "    \"Medicine_Name\": x[\"Medicine_Name\"],\n",
    "    \"Patient_Id\": x[\"Patient_Id\"],\n",
    "    \"Gender\" : x[\"Gender\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med_dataset_select = med_dataset.map(lambda x: x[\"Medicine_Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = patient_dataset_select.shuffle(1000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(800)\n",
    "test = shuffled.skip(200).take(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'A Doxid 100mg Capsule', b'A Ret HC Cream', b'A-CN Gel',\n",
       "       b'A-Ret 0.025% Gel', b'A-Ret 0.05% Gel', b'A-Ret 0.1% Gel',\n",
       "       b'A-Ret 0.5% Cream', b'AA 5 Tablet', b'AB Phylline Capsule',\n",
       "       b'AB Phylline N Tablet'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_titles = med_dataset_setlect.batch(1_000)\n",
    "patient_dataset_ids = patient_dataset_select.batch(1_000).map(lambda x: x[\"Patient_Id\"])\n",
    "\n",
    "unique_med_titles = np.unique(np.concatenate(list(med_titles)))\n",
    "unique_patient_dataset_ids = np.unique(np.concatenate(list(patient_dataset_ids)))\n",
    "\n",
    "unique_med_titles[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'10001', b'10002', b'10003', b'10004', b'10005', b'10006',\n",
       "       b'10007', b'10008', b'10009', b'10010'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_patient_dataset_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_patient_dataset_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementing a model\n",
    "\n",
    "Choosing the architecture of our model is a key part of modelling.\n",
    "\n",
    "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The query tower\n",
    "\n",
    "Let's start with the query tower.\n",
    "\n",
    "The first step is to decide on the dimensionality of the query and candidate representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_med_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_patient_dataset_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_patient_dataset_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The candidate tower\n",
    "\n",
    "We can do the same with the candidate tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_med_titles, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_med_titles) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In our training data we have positive (user, medication) pairs. To figure out how good our model is, we need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
    "\n",
    "To do this, we can use the `tfrs.metrics.FactorizedTopK` metric. The metric has one required argument: the dataset of candidates that are used as implicit negatives for evaluation.\n",
    "\n",
    "In our case, that's the `meds` dataset, converted into embeddings via our movie model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=med_dataset_select.batch(128).map(med_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "The next component is the loss used to train our model. TFRS has several loss layers and tasks to make this easy.\n",
    "\n",
    "In this instance, we'll make use of the `Retrieval` task object: a convenience wrapper that bundles together the loss function and metric computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full model\n",
    "\n",
    "We can now put it all together into a model. \n",
    "TFRS exposes a base model class (`tfrs.models.Model`) which streamlines building models: all we need to do is to set up the components in the `__init__` method, and implement the `compute_loss` method, taking in the raw features and returning a loss value.\n",
    "\n",
    "The base model will then take care of creating the appropriate training loop to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MedRecomModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_med_model, med_model):\n",
    "    super().__init__()\n",
    "    self.med_model: tf.keras.Model = med_model\n",
    "    self.user_med_model: tf.keras.Model = user_med_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_med_embeddings = self.user_med_model(features[\"Patient_Id\"])\n",
    "    # And pick out the med features and pass them into the med model,\n",
    "    # getting embeddings back.\n",
    "    positive_med_embeddings = self.med_model(features[\"Medicine_Name\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_med_embeddings, positive_med_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fitting and evaluating\n",
    "\n",
    "After defining the model, we can use standard Keras fitting and evaluation routines to fit and evaluate the model.\n",
    "\n",
    "Let's first instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MedRecomModel(user_med_model, med_model)\n",
    "#Pick righ optimizer for training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then shuffle, batch, and cache the training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(1000).batch(10).cache()\n",
    "cached_test = test.batch(100).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "80/80 [==============================] - 16s 174ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0538 - factorized_top_k/top_5_categorical_accuracy: 0.0712 - factorized_top_k/top_10_categorical_accuracy: 0.0875 - factorized_top_k/top_50_categorical_accuracy: 0.1112 - factorized_top_k/top_100_categorical_accuracy: 0.1375 - loss: 23.0244 - regularization_loss: 0.0000e+00 - total_loss: 23.0244\n",
      "Epoch 2/3\n",
      "80/80 [==============================] - 13s 168ms/step - factorized_top_k/top_1_categorical_accuracy: 0.4663 - factorized_top_k/top_5_categorical_accuracy: 0.5462 - factorized_top_k/top_10_categorical_accuracy: 0.5663 - factorized_top_k/top_50_categorical_accuracy: 0.6212 - factorized_top_k/top_100_categorical_accuracy: 0.6463 - loss: 22.7042 - regularization_loss: 0.0000e+00 - total_loss: 22.7042\n",
      "Epoch 3/3\n",
      "80/80 [==============================] - 13s 166ms/step - factorized_top_k/top_1_categorical_accuracy: 0.7000 - factorized_top_k/top_5_categorical_accuracy: 0.7713 - factorized_top_k/top_10_categorical_accuracy: 0.7875 - factorized_top_k/top_50_categorical_accuracy: 0.8450 - factorized_top_k/top_100_categorical_accuracy: 0.8737 - loss: 20.5719 - regularization_loss: 0.0000e+00 - total_loss: 20.5719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 86848), started 20:44:34 ago. (Use '!kill 86848' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-65ef8fd2c1e5ab79\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-65ef8fd2c1e5ab79\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3,callbacks=[tensorboard_callback])\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cached_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 270ms/step - factorized_top_k/top_1_categorical_accuracy: 0.7850 - factorized_top_k/top_5_categorical_accuracy: 0.8450 - factorized_top_k/top_10_categorical_accuracy: 0.8750 - factorized_top_k/top_50_categorical_accuracy: 0.9200 - factorized_top_k/top_100_categorical_accuracy: 0.9400 - loss: 415.6057 - regularization_loss: 0.0000e+00 - total_loss: 415.6057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.7850000262260437,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.8450000286102295,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.875,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.9200000166893005,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.9399999976158142,\n",
       " 'loss': 414.11175537109375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 414.11175537109375}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making predictions\n",
    "\n",
    "Now that we have a model, we would like to be able to make predictions. We can use the `tfrs.layers.factorized_top_k.BruteForce` layer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 10025: [b'Myket Cream' b'Maskofung-XL Cream' b'Medrol 4mg Tablet'\n",
      " b'Forfora-Z Shampoo' b'Oxerute Cream' b'Omnitan H Tablet'\n",
      " b'Eptoin 300 ER Tablet' b'Dresin Mouthwash &amp; Gargle' b'Ondem Syrup'\n",
      " b'Monoguard-B Cream']\n"
     ]
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_med_model)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((med_dataset_select.batch(100), med_dataset_select.batch(100).map(model.med_model)))\n",
    ")\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = index(tf.constant([\"10025\"]))\n",
    "print(f\"Recommendations for user 10025: {titles[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model serving\n",
    "\n",
    "After the model is trained, we need a way to deploy it.\n",
    "\n",
    "In a two-tower retrieval model, serving has two components:\n",
    "\n",
    "- a serving query model, taking in features of the query and transforming them into a query embedding, and\n",
    "- a serving candidate model. This most often takes the form of an approximate nearest neighbours (ANN) index which allows fast approximate lookup of candidates in response to a query produced by the query model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /var/tmp/tmp4l5w4yqn/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: [b'Olmetor-H Tablet' b'Ramistar 2.5 Tablet' b'Quel SR 200 Tablet']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Export the query model.\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "  path = os.path.join(tmp, \"model\")\n",
    "\n",
    "  # Save the index.\n",
    "  tf.saved_model.save(index, path)\n",
    "\n",
    "  # Load it back; can also be done in TensorFlow Serving.\n",
    "  loaded = tf.saved_model.load(path)\n",
    "\n",
    "  # Pass a user id in, get top predicted movie titles back.\n",
    "  scores, titles = loaded([\"10042\"])\n",
    "\n",
    "  print(f\"Recommendations: {titles[0][:3]}\")\n",
    "  dst_path = \"/tmp/vc-model-med-select-recomm-2\"\n",
    "  os.rename(path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/vc-model-med-select-recomm-2/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "Copying file:///tmp/vc-model-med-select-recomm-2/fingerprint.pb [Content-Type=application/octet-stream]...\n",
      "Copying file:///tmp/vc-model-med-select-recomm-2/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "Copying file:///tmp/vc-model-med-select-recomm-2/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "- [4 files][  1.4 MiB/  1.4 MiB]                                                \n",
      "Operation completed over 4 objects/1.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp -r \"/tmp/vc-model-med-select-recomm-2\" \"gs://vc-model-training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete endpoint resource\n",
    "# e.g. `endpoint.delete()`\n",
    "\n",
    "# Delete model resource\n",
    "# e.g. `model.delete()`\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = True\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
